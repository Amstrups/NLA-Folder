{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\author{Christian Amstrup Petersen}\n",
    "By Christian Amstrup Petersen,\n",
    "Student number: 202104742\n",
    "\\appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're given the following matrix:\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 && 0 && 0 \\\\ 0 && 1 && 1 \\\\ 0 && 1 && 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{The characteristic polynomial}\n",
    "We here want to find the characteristic polynomial $p$ for $A$. We get this by using proposition 21.15, so:\n",
    "$$p(A) = det(A-\\lambda I_3)$$\n",
    "Following definition 21.12, we notice that parts of the equation is nulified, due to $A_{01} = 0$ and $A_{02} = 2$, so we are left with:\n",
    "$$p(A) = (2-\\lambda)det \\begin{bmatrix} 1-\\lambda && 1 \\\\ 1 && 1-\\lambda \\end{bmatrix}$$\n",
    "$$= (2-\\lambda)(\\lambda^2 -2\\lambda)$$\n",
    "$$= -\\lambda^3+4\\lambda^2-4\\lambda$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Eigenvalues}\n",
    "We're asked to show that 0 and 2 are Eigenvalues to $A$. To determine our eigenvalues normally, we would have to solve for the roots of $p(A)$ (See prop. 21.2), but we are now asked to verify instead. This simplifies the process, as we can just set $\\lambda$ equal to one of our given values, and checking if $p(A)$ then equals 0. For $\\lambda = 0$:\n",
    "$$-0^3+4\\cdot0^2-4\\cdot0 = 0$$\n",
    "We can conclude that 0 is an eigenvalue for $A$. Now for $\\lambda = 2$:\n",
    "$$-2^3+4\\cdot2^2-4\\cdot2 = 0$$\n",
    "$$-8+16-8=0$$\n",
    "Therefore 2 is also an eigenvalue for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{More(!) Eigenvalues! (Maybe)}\n",
    "As stated above, we could find the Eigenvalues by solving $p(A)$ for its roots, so let us just do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = [-1, 4, -4, 0]\n",
    "eigen = np.roots(coeff)\n",
    "eigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have a dublicate root of 2, this changes nothing for what we proved in assignment $b$, and we can thus conclude, that only $\\lambda_0 = 0$ and $\\lambda_1 = 2$ (and an added $\\lambda_2 = 2$) are eigenvalues for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Eigen-va... I mean -vectors}\n",
    "We now turn our eyes to definition 21.1. Using our $\\lambda_0$, $\\lambda_1$ and $\\lambda_2$, we now want to determinte our eigenvectors $v_0$, $v_1$, and $v_2$. Firstly, for also later use, we denote the following:\n",
    "\n",
    "We take the following formula from the definition:\n",
    "$$(A-\\lambda_x I_n)v_x = 0$$\n",
    "From doing some 'r√¶kke'-operations on any given $(A-\\lambda_x I_3)$, we can contruct an echelon-matrix, from which we can create our $v_x$.\n",
    "So for $lambda_0$, we just get $(A-\\lambda_0 I_3) = A$, and due to $A_{R1} = A_{R2}$, we subtract one from the other and therefore get:\n",
    "$$\\begin{bmatrix} 2 && 0 && 0 \\\\ 0 && 1 && 1 \\\\ 0 && 0 && 0 \\end{bmatrix}v_0 = 0$$\n",
    "$$\\Rightarrow v_0 = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "And for $\\lambda_1$ and $\\lambda_2$, we get:\n",
    "$$\\begin{bmatrix} 2 && 0 && 0 \\\\ 0 && 1 && 1 \\\\ 0 && 1 && 1 \\end{bmatrix}-\\begin{bmatrix} 2 && 0 && 0 \\\\ 0 && 2 && 0 \\\\ 0 && 0 && 2 \\end{bmatrix} = \\begin{bmatrix} 0 && 0 && 0 \\\\ 0 && -1 && 1 \\\\ 0 && 1 && -1 \\end{bmatrix}$$\n",
    "Here we split up the calculations for $v_1$ and $v_2$. Firstly\n",
    "$$\\begin{bmatrix} 0 && 0 && 0 \\\\ 0 && -1 && 1 \\\\ 0 && 1 && -1 \\end{bmatrix}v_1 = 0$$\n",
    "Here we eliminate one of the remaining rows by adding the other to it, so we're left with:\n",
    "$$\\begin{bmatrix} 0 && 0 && 0 \\\\ 0 && 0 && 0 \\\\ 0 && 1 && -1 \\end{bmatrix}v_1 = 0$$\n",
    "$$v_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$$\n",
    "For $v_2$, we use a piece of advice from Mr. Swann, as we have a row of free variables, and we can simply set the related entrance in $v_2$ to 1, and leave the rest as zeros, so we get:\n",
    "$$\\begin{bmatrix} 0 && 0 && 0 \\\\ 0 && -1 && 1 \\\\ 0 && 1 && -1 \\end{bmatrix}\\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} = 0$$\n",
    "$$\\Rightarrow v_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n",
    "We can now denote this collection as\n",
    "$$V = \\begin{bmatrix} v_0 | v_1 | v_2 \\end{bmatrix}$$\n",
    "Taking a look at prop. 21.5, stating that eigenvectors are lineary independent, we fulfill the first requirement in def. 19.1, for $V$ being a basis in $\\mathbb{R}^3$. For the other requirement, we know that we're dealing a with a kvadratic matrix, and thus use prop. 19.4, so we can conclude that $V$ is a basis for $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{The V, the upside one, and the inverted}\n",
    "We now want to chooce fitting variables for the equation:\n",
    "$$A = V\\Lambda V^{-1}$$\n",
    "But this is just the equation seen in prop 21.6, and to all we need to do is introduce\n",
    "$$\\Lambda = diag(\\lambda_0, \\lambda_1, \\lambda_2) = diag(0,2,2),$$\n",
    "and use numpy to calculate the inverted $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.5, -0.5],\n",
       "       [ 0. ,  0.5,  0.5],\n",
       "       [ 1. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "V = np.array([[0.,0.,1.], [1.,1.,0.], [-1.,1.,0.]])\n",
    "np.linalg.inv(V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So\n",
    "$$\\begin{bmatrix} 0 && 0 && 1 \\\\ 1 && 1 && 0 \\\\ -1 && 1 && 0 \\end{bmatrix}\\begin{bmatrix} 0 && 0 && 0 \\\\ 0 && 2 && 0 \\\\ 0 && 0 && 2 \\end{bmatrix}\\begin{bmatrix} 0 && 0.5 && -0.5 \\\\ 0 && 0.5 && 0.5 \\\\ 1 && 0 && 0 \\end{bmatrix} = \\begin{bmatrix} 2 && 0 && 0 \\\\ 0 && 1 && 1 \\\\ 0 && 1 && 1 \\end{bmatrix} = A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{It's not a Markov-chain}\n",
    "For this part of the handin, we're asked to decide $A^k$ for all $k$. And thanks to dear Mr. Swann and his wonderful notes, it's a fairly easy job. Taking a look just above formula 20.3, we get the formula:\n",
    "$$A^k = T C^k T^{-1},$$\n",
    "which resembles our little equation from assignment ($e$) in this handin, with the only requirement for this to hold, being that $C$ is a diagonal matrix. Following this formula, switching out with our own values, we get that\n",
    "$$A^k = V \\Lambda^k V^{-1}$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
